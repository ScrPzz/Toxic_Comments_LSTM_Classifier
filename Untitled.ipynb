{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clean_text import clean_text\n",
    "from process import process\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import re  # library for regular expression operations\n",
    "import string\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import string  # for string operations\n",
    "\n",
    "from nltk.corpus import stopwords  # module for stop words that come with NLTK\n",
    "from nltk.stem import PorterStemmer  # module for stemming\n",
    "from nltk.tokenize import regexp_tokenize  # module for tokenizing strings\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "import nltk  # Python library for NLP\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\")\n",
    "import sys\n",
    "sys.setrecursionlimit(3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re  # library for regular expression operations\n",
    "\n",
    "import string  # for string operations\n",
    "\n",
    "def clean_text(text):\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    # remove stock market tickers like $GE\n",
    "    text = re.sub(r'\\$\\w*', '', text)\n",
    "    text = re.sub(r'^RT[\\s]+', '', text)\n",
    "    # remove hyperlinks\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    text = re.sub(r'#', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   def stem_and_stopwords(comment):\n",
    "    stemmer = PorterStemmer()\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    \n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "    comment_tokens = tokenizer.tokenize(comment)\n",
    "    #print(comment_tokens)\n",
    "    comments_clean = []\n",
    "    for word in comment_tokens:\n",
    "       if (word not in stopwords_english and  # remove stopwords\n",
    "                word not in string.punctuation):  # remove punctuation\n",
    "         # tweets_clean.append(word)\n",
    "         stem_word = stemmer.stem(word)  # stemming word\n",
    "         comments_clean.append(stem_word)\n",
    "    return comments_clean  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "test_subm = pd.read_csv('sample_submission.csv')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['comment_text'] = train['comment_text'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "words = ' '.join([text for text in train[\"comment_text\"]])\n",
    "\n",
    "word_cloud = WordCloud(\n",
    "    width=1600,\n",
    "    height=800,\n",
    "    # colormap='PuRd',\n",
    "    margin=0,\n",
    "    max_words=100,  # Maximum numbers of words we want to see\n",
    "    min_word_length=3,  # Minimum numbers of letters of each word to be part of the cloud\n",
    "    max_font_size=150, min_font_size=30,  # Font size range\n",
    "    background_color=\"white\").generate(words)\n",
    "\n",
    "plt.figure(figsize=(10, 16))\n",
    "plt.imshow(word_cloud, interpolation=\"gaussian\")\n",
    "plt.title('Comments and their Nature', fontsize=40)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definig a function to remove the stopwords\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    \n",
    "    words = [word for word in text if word not in stopwords.words('english')]\n",
    "    return words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['comment_text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Applying the remove_stopwords on train and test set\n",
    "\n",
    " train['StemStop'] = train['comment_text'].apply(lambda x: stem_and_stopwords(x))\n",
    " test['StemStop'] = test['comment_text'].apply(lambda x: stem_and_stopwords(x))\n",
    "\n",
    "\n",
    " train.head()\n",
    " test.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "targets = train[cols].values\n",
    "\n",
    "train_df = train['comment_text']\n",
    "test_df = test['comment_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_counts = train[cols].sum()\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "ax = sns.barplot(val_counts.index, val_counts.values, alpha=0.8)\n",
    "\n",
    "plt.title(\"Comments per Classes\")\n",
    "plt.xlabel(\"Comment label\")\n",
    "plt.ylabel(\"Count\")\n",
    "\n",
    "rects = ax.patches\n",
    "labels = val_counts.values\n",
    "for rect, label in zip(rects, labels):\n",
    "    height = rect.get_height()\n",
    "    ax.text(rect.get_x() + rect.get_width()/2, height+5, label, ha=\"center\", va=\"bottom\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "words = ' '.join([text for text in train['comment_text'] ])\n",
    "\n",
    "\n",
    "word_cloud = WordCloud(\n",
    "                       width=1600,\n",
    "                       height=800,\n",
    "                       #colormap='PuRd', \n",
    "                       margin=0,\n",
    "                       max_words=500, # Maximum numbers of words we want to see \n",
    "                       min_word_length=3, # Minimum numbers of letters of each word to be part of the cloud\n",
    "                       max_font_size=150, min_font_size=30,  # Font size range\n",
    "                       background_color=\"white\").generate(words)\n",
    "\n",
    "plt.figure(figsize=(10, 16))\n",
    "plt.imshow(word_cloud, interpolation=\"gaussian\")\n",
    "plt.title('Comments and their Nature', fontsize = 40)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Cloud for test set\n",
    "\n",
    "words = ' '.join([text for text in test['comment_text'] ])\n",
    "\n",
    "\n",
    "word_cloud = WordCloud(\n",
    "                       width=1600,\n",
    "                       height=800,\n",
    "                       #colormap='PuRd', \n",
    "                       margin=0,\n",
    "                       max_words=500, # Maximum numbers of words we want to see \n",
    "                       min_word_length=3, # Minimum numbers of letters of each word to be part of the cloud\n",
    "                       max_font_size=150, min_font_size=30,  # Font size range\n",
    "                       background_color=\"white\").generate(words)\n",
    "\n",
    "plt.figure(figsize=(10, 16))\n",
    "plt.imshow(word_cloud, interpolation=\"bilinear\")\n",
    "plt.title('Comments and their Nature', fontsize = 40)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "max_features = 22000\n",
    "\n",
    "tokenizer = Tokenizer(num_words = max_features)\n",
    "tokenizer.fit_on_texts(list(train_df))\n",
    "\n",
    "tokenized_train = tokenizer.texts_to_sequences(train_df)\n",
    "tokenized_test = tokenizer.texts_to_sequences(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import initializers, optimizers, layers\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "maxlen = 200\n",
    "X_train = pad_sequences(tokenized_train, maxlen = maxlen)\n",
    "X_test = pad_sequences(tokenized_test, maxlen = maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, Input,  Activation\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import initializers, optimizers, layers\n",
    "from sklearn.metrics import roc_auc_score\n",
    "embed_size = 128\n",
    "maxlen = 200\n",
    "max_features = 22000\n",
    "\n",
    "inp = Input(shape = (maxlen, ))\n",
    "x = Embedding(max_features, embed_size)(inp)\n",
    "x = LSTM(60, return_sequences=True, name='lstm_layer')(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(50, activation=\"relu\")(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(6, activation=\"sigmoid\")(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(\n",
    "loss='binary_crossentropy',\n",
    "optimizer='adam',\n",
    "metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 5\n",
    "model.fit(X_train, targets, batch_size=batch_size, epochs=epochs, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(X_test)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
